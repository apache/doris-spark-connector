// Licensed to the Apache Software Foundation (ASF) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

package org.apache.doris.spark.util

import com.fasterxml.jackson.databind.SerializationFeature
import com.fasterxml.jackson.databind.json.JsonMapper
import com.fasterxml.jackson.module.scala.DefaultScalaModule
import org.apache.spark.sql.catalyst.InternalRow
import org.apache.spark.sql.catalyst.expressions.SpecializedGetters
import org.apache.spark.sql.catalyst.util.{ArrayBasedMapData, ArrayData, DateTimeUtils}
import org.apache.spark.sql.types._
import org.apache.spark.unsafe.types.UTF8String

import java.sql.{Date, Timestamp}
import java.time.{Instant, LocalDate, LocalDateTime, ZoneId}
import java.util
import scala.collection.JavaConverters.{asScalaBufferConverter, mapAsScalaMapConverter, seqAsJavaListConverter}
import scala.collection.mutable

object RowConvertors {

  private val MAPPER = JsonMapper.builder().addModule(DefaultScalaModule)
    .configure(SerializationFeature.ORDER_MAP_ENTRIES_BY_KEYS, true).build()

  private val NULL_VALUE = "\\N"

  def convertToCsv(row: InternalRow, schema: StructType, sep: String): String = {
    (0 until schema.length).map(i => {
      val value = asScalaValue(row, schema.fields(i).dataType, i)
      if (value == null) NULL_VALUE else value
    }).mkString(sep)
  }

  def convertToJson(row: InternalRow, schema: StructType): String = {
    val map: util.HashMap[String, Any] = convertRowToMap(row, schema)
    MAPPER.writeValueAsString(map)
  }

  private def convertRowToMap(row: InternalRow, schema: StructType) = {
    val map = new util.HashMap[String, Any](schema.fields.size)
    (0 until schema.length).foreach(i => {
      map.put(schema.fields(i).name, asScalaValue(row, schema.fields(i).dataType, i))
    })
    map
  }

  def convertToJsonBytes(row: InternalRow, schema: StructType): Array[Byte] = {
    val map: util.HashMap[String, Any] = convertRowToMap(row, schema)
    MAPPER.writeValueAsBytes(map)
  }

  private def asScalaValue(row: SpecializedGetters, dataType: DataType, ordinal: Int): Any = {
    if (row.isNullAt(ordinal)) null
    else {
      dataType match {
        case NullType => NULL_VALUE
        case BooleanType => row.getBoolean(ordinal)
        case ByteType => row.getByte(ordinal)
        case ShortType => row.getShort(ordinal)
        case IntegerType => row.getInt(ordinal)
        case LongType => row.getLong(ordinal)
        case FloatType => row.getFloat(ordinal)
        case DoubleType => row.getDouble(ordinal)
        case StringType => Option(row.getUTF8String(ordinal)).map(_.toString).getOrElse(NULL_VALUE)
        case TimestampType =>
          DateTimeUtils.toJavaTimestamp(row.getLong(ordinal)).toString
        case DateType => DateTimeUtils.toJavaDate(row.getInt(ordinal)).toString
        case BinaryType => row.getBinary(ordinal)
        case dt: DecimalType => row.getDecimal(ordinal, dt.precision, dt.scale).toJavaBigDecimal
        case at: ArrayType =>
          val arrayData = row.getArray(ordinal)
          if (arrayData == null) NULL_VALUE
          else {
            (0 until arrayData.numElements()).map(i => {
              if (arrayData.isNullAt(i)) null else asScalaValue(arrayData, at.elementType, i)
            }).mkString("[", ",", "]")
          }
        case mt: MapType =>
          val mapData = row.getMap(ordinal)
          if (mapData.numElements() == 0) "{}"
          else {
            val keys = mapData.keyArray()
            val values = mapData.valueArray()
            val map = mutable.HashMap[Any, Any]()
            var i = 0
            while (i < keys.numElements()) {
              map += asScalaValue(keys, mt.keyType, i) -> asScalaValue(values, mt.valueType, i)
              i += 1
            }
            MAPPER.writeValueAsString(map)
          }
        case st: StructType =>
          val structData = row.getStruct(ordinal, st.length)
          val map = new java.util.TreeMap[String, Any]()
          var i = 0
          while (i < structData.numFields) {
            val field = st.fields(i)
            map.put(field.name, asScalaValue(structData, field.dataType, i))
            i += 1
          }
          MAPPER.writeValueAsString(map)
        case _ => throw new Exception(s"Unsupported spark type: ${dataType.typeName}")
      }
    }
  }

  def convertValue(v: Any, dataType: DataType, datetimeJava8ApiEnabled: Boolean): Any = {
    dataType match {
      case StringType => UTF8String.fromString(v.asInstanceOf[String])
      case TimestampType if datetimeJava8ApiEnabled => DateTimeUtils.instantToMicros(v.asInstanceOf[Instant])
      case TimestampType => DateTimeUtils.fromJavaTimestamp(v.asInstanceOf[Timestamp])
      case DateType if datetimeJava8ApiEnabled => v.asInstanceOf[LocalDate].toEpochDay.toInt
      case DateType => DateTimeUtils.fromJavaDate(v.asInstanceOf[Date])
      case _: MapType =>
        val map = v.asInstanceOf[java.util.Map[String, String]].asScala
        val keys = map.keys.toArray.map(UTF8String.fromString)
        val values = map.values.toArray.map(UTF8String.fromString)
        ArrayBasedMapData(keys, values)
      case at: ArrayType =>
        // ARRAY data from RowBatch comes as JSON string, convert back to List
        // Examples: "[\"Alice\",\"Bob\"]" or "[1,2,3]"
        val inputValue = v match {
          case s: String =>
            // Parse JSON string to List
            try {
              MAPPER.readValue(s, classOf[java.util.List[Any]])
            } catch {
              case _: Exception =>
                // Fallback: return empty list if JSON parsing fails
                new java.util.ArrayList[Any]()
            }
          case list: java.util.List[Any] =>
            // Already a List (e.g., from direct conversion path)
            list
          case _ =>
            // Unexpected type, return empty list
            new java.util.ArrayList[Any]()
        }
        
        // Convert Java List to Spark ArrayData
        // Performance optimization: Pre-allocate array with known size
        val javaList = inputValue.asInstanceOf[java.util.List[Any]]
        val listSize = javaList.size()
        val elements = new Array[Any](listSize)
        var i = 0
        val iterator = javaList.iterator()
        while (iterator.hasNext) {
          val element = iterator.next()
          if (element == null) {
            elements(i) = null
          } else {
            // Recursively convert element based on element type
            elements(i) = convertArrayElement(element, at.elementType, datetimeJava8ApiEnabled)
          }
          i += 1
        }
        
        // Create ArrayData based on element type with proper type conversion
        // If schema declares StringType but actual data is another type, convert to string
        try {
          at.elementType match {
            case BooleanType => ArrayData.toArrayData(elements.map(_.asInstanceOf[Boolean]))
            case ByteType => ArrayData.toArrayData(elements.map(_.asInstanceOf[Byte]))
            case ShortType => ArrayData.toArrayData(elements.map(_.asInstanceOf[Short]))
            case IntegerType => ArrayData.toArrayData(elements.map(_.asInstanceOf[Int]))
            case LongType => ArrayData.toArrayData(elements.map(_.asInstanceOf[Long]))
            case FloatType => ArrayData.toArrayData(elements.map(_.asInstanceOf[Float]))
            case DoubleType => ArrayData.toArrayData(elements.map(_.asInstanceOf[Double]))
            case StringType => ArrayData.toArrayData(elements.map(e => UTF8String.fromString(e.toString)))
            case _ => ArrayData.toArrayData(elements)
          }
        } catch {
          case _: ClassCastException =>
            // Fallback: if type conversion fails, convert to string array
            // This handles cases where schema declares a specific type but actual data is another type
            // Common scenario: Schema declares StringType (default) but actual data is IntegerType, etc.
            ArrayData.toArrayData(elements.map(e => UTF8String.fromString(e.toString)))
        }
      case NullType | BooleanType | ByteType | ShortType | IntegerType | LongType | FloatType | DoubleType | BinaryType | _: DecimalType => v
      case _ => throw new Exception(s"Unsupported spark type: ${dataType.typeName}")
    }
  }

  /**
   * Recursively convert array element based on element type
   * Supports nested arrays and all primitive types with proper type conversion
   * 
   * @param element the element value to convert
   * @param elementType the expected Spark DataType for the element
   * @param datetimeJava8ApiEnabled whether to use Java 8 date/time API
   * @return converted element value suitable for Spark InternalRow
   */
  private def convertArrayElement(element: Any, elementType: DataType, datetimeJava8ApiEnabled: Boolean): Any = {
    elementType match {
      case StringType => UTF8String.fromString(element.toString)
      case TimestampType if datetimeJava8ApiEnabled && element.isInstanceOf[LocalDateTime] =>
        val localDateTime = element.asInstanceOf[LocalDateTime]
        val instant = localDateTime.atZone(ZoneId.systemDefault()).toInstant
        DateTimeUtils.instantToMicros(instant)
      case TimestampType if element.isInstanceOf[Timestamp] =>
        DateTimeUtils.fromJavaTimestamp(element.asInstanceOf[Timestamp])
      case DateType if datetimeJava8ApiEnabled && element.isInstanceOf[LocalDate] =>
        element.asInstanceOf[LocalDate].toEpochDay.toInt
      case DateType if element.isInstanceOf[Date] =>
        DateTimeUtils.fromJavaDate(element.asInstanceOf[Date])
      case nestedArray: ArrayType =>
        // Handle nested arrays recursively
        val nestedJavaList = element.asInstanceOf[java.util.List[Any]]
        val nestedListSize = nestedJavaList.size()
        val nestedElements = new Array[Any](nestedListSize)
        var j = 0
        val nestedIterator = nestedJavaList.iterator()
        while (nestedIterator.hasNext) {
          val e = nestedIterator.next()
          nestedElements(j) = convertArrayElement(e, nestedArray.elementType, datetimeJava8ApiEnabled)
          j += 1
        }
        ArrayData.toArrayData(nestedElements)
      case _ => element
    }
  }

}
